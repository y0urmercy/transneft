# docx_parser_fixed.py
from docx import Document
import pandas as pd
import json
import re
from pathlib import Path
import logging
from typing import List, Dict
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TransneftDocxParser:
    def __init__(self, docx_path: str):
        self.docx_path = docx_path
        self.document = None
        self.sections = {}
        self.qa_triplets = []

    def load_document(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ DOCX –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            self.document = Document(self.docx_path)
            logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {self.docx_path}")
            logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(self.document.paragraphs)}")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False

    def extract_structure(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        sections = {}
        current_section = "–í–≤–µ–¥–µ–Ω–∏–µ"
        current_content = []

        for i, paragraph in enumerate(self.document.paragraphs):
            text = paragraph.text.strip()
            if not text:
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ —Å—Ç–∏–ª—é –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
            if self._is_heading(paragraph, text):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–¥–µ–ª
                if current_content and len(' '.join(current_content)) > 50:
                    content_text = ' '.join(current_content)
                    sections[current_section] = {
                        'content': content_text,
                        'paragraphs': len(current_content),
                        'word_count': len(content_text.split())
                    }

                current_section = text
                current_content = []
            else:
                current_content.append(text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_content and len(' '.join(current_content)) > 50:
            content_text = ' '.join(current_content)
            sections[current_section] = {
                'content': content_text,
                'paragraphs': len(current_content),
                'word_count': len(content_text.split())
            }

        self.sections = sections
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {len(sections)}")
        return sections

    def _is_heading(self, paragraph, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º"""
        # –ü–æ —Å—Ç–∏–ª—é
        if hasattr(paragraph, 'style') and paragraph.style.name.startswith('Heading'):
            return True

        # –ü–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é (–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ü–µ)
        if len(text) < 200 and not text.endswith(('.', '!', '?')):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            if (text.isupper() or
                    any(word in text.lower() for word in ['—Ä–∞–∑–¥–µ–ª', '–≥–ª–∞–≤–∞', '—á–∞—Å—Ç—å', '¬ß']) or
                    re.match(r'^\d+[\.\)]', text) or  # "1.", "1)"
                    re.match(r'^[IVXLCDM]+\.', text) or  # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã
                    re.match(r'^[–ê-–Ø][–∞-—è]+\s*$', text)):  # –û–¥–Ω–æ —Å–ª–æ–≤–æ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
                return True

        return False

    def extract_entities(self, text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å"""
        entities = []

        # –î–∞—Ç—ã
        dates = re.findall(r'\b\d{1,2}\.\d{1,2}\.\d{4}\b', text)
        for date in dates:
            entities.append({'type': 'date', 'value': date})

        # –ì–æ–¥—ã
        years = re.findall(r'\b(?:19|20)\d{2}\b', text)
        for year in years:
            entities.append({'type': 'year', 'value': year})

        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        financials = re.findall(
            r'\b\d+(?:\s*\d{3})*(?:\,\d+)?\s*(?:–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å|—Ç–æ–Ω–Ω|–∫–º|—Ä—É–±–ª–µ–π|—Ä—É–±|–∞–∫—Ü–∏–π)(?:\s*\/\s*–≥–æ–¥)?\b',
            text,
            re.IGNORECASE
        )
        for financial in financials:
            entities.append({'type': 'financial', 'value': financial})

        # –ü—Ä–æ–µ–∫—Ç—ã –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å
        projects = re.findall(
            r'(?:–ø—Ä–æ–µ–∫—Ç|–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–≤–æ–¥|—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥|—Å–∏—Å—Ç–µ–º–∞)\s+[¬´"]([^¬ª"]+)[¬ª"]',
            text
        )
        for project in projects:
            entities.append({'type': 'project', 'value': project})

        # –ö–ª—é—á–µ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã
        objects = re.findall(
            r'\b(?:–í–°–¢–û|–ë–¢–°|–ö–¢–ö|–ó–∞–ø–æ–ª—è—Ä—å–µ|–ö—É—é–º–±–∞|–¢–∞–π—à–µ—Ç|–°–∫–æ–≤–æ—Ä–æ–¥–∏–Ω–æ|–ö–æ–∑—å–º–∏–Ω–æ|–ü—Ä–∏–º–æ—Ä—Å–∫|–£—Å—Ç—å-–õ—É–≥–∞)\b',
            text
        )
        for obj in objects:
            entities.append({'type': 'object', 'value': obj})

        return entities

    def generate_section_questions(self, section_title: str, content: str, entities: List[Dict]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞"""
        questions = []

        content_lower = content.lower()
        section_lower = section_title.lower()

        # –í–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ä–∞–∑–¥–µ–ª–∞
        if any(word in section_lower for word in ['—É—Å—Ç–∞–≤–Ω—ã–π', '–∫–∞–ø–∏—Ç–∞–ª', '–∞–∫—Ü–∏']):
            questions.extend([
                "–ö–∞–∫–æ–≤ —Ä–∞–∑–º–µ—Ä —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ –ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å?",
                "–ò–∑ –∫–∞–∫–∏—Ö –∞–∫—Ü–∏–π —Å–æ—Å—Ç–æ–∏—Ç —É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª?",
                "–ö–∞–∫ –∏–∑–º–µ–Ω—è–ª—Å—è —É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª –∫–æ–º–ø–∞–Ω–∏–∏?",
            ])

        elif any(word in section_lower for word in ['–∏—Å—Ç–æ—Ä–∏', '–æ—Å–Ω–æ–≤–∞–Ω']):
            questions.extend([
                "–ö–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –∫–æ–º–ø–∞–Ω–∏—è –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å?",
                "–ö–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –≤–µ—Ö–∏ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏?",
                "–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ —Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏?",
            ])

        elif any(word in section_lower for word in ['–ø—Ä–æ–µ–∫—Ç', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥', '–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–≤–æ–¥']):
            project_entities = [e['value'] for e in entities if e['type'] == 'project']
            if project_entities:
                for project in project_entities[:2]:
                    questions.extend([
                        f"–í —á–µ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–æ–µ–∫—Ç '{project}'?",
                        f"–ö–∞–∫–∏–µ —Ü–µ–ª–∏ —É –ø—Ä–æ–µ–∫—Ç–∞ '{project}'?",
                        f"–ö–∞–∫–æ–≤—ã —Å—Ä–æ–∫–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ '{project}'?",
                    ])
            else:
                questions.extend([
                    "–û –∫–∞–∫–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∏–¥–µ—Ç —Ä–µ—á—å –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ?",
                    "–ö–∞–∫–∏–µ —Ü–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ —É —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞?",
                    "–ö–∞–∫–æ–≤—ã –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞?",
                ])

        elif any(word in content_lower for word in ['–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–æ–≤–µ—Ç']):
            questions.extend([
                "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å?",
                "–ö–∞–∫–∏–µ –æ—Ä–≥–∞–Ω—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ –∫–æ–º–ø–∞–Ω–∏–∏?",
                "–ö—Ç–æ –≤—Ö–æ–¥–∏—Ç –≤ —Å–æ—Å—Ç–∞–≤ –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤?",
            ])

        # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        questions.extend([
            f"–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{section_title}'?",
            f"–û —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{section_title}'?",
            f"–ö–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ä–∞–∑–¥–µ–ª–µ '{section_title}'?",
        ])

        return list(set(questions))[:4]  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 4 –≤–æ–ø—Ä–æ—Å–∞–º–∏

    def create_qa_triplets(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ QA —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤"""
        self.qa_triplets = []

        for section_title, section_data in self.sections.items():
            content = section_data['content']

            if len(content) < 100:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã
                continue

            entities = self.extract_entities(content)
            questions = self.generate_section_questions(section_title, content, entities)

            for i, question in enumerate(questions):
                # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞
                answer = self._generate_targeted_answer(content, question)

                self.qa_triplets.append({
                    'section': section_title,
                    'context': content,
                    'question': question,
                    'answer': answer,
                    'question_id': f"{section_title}_{i}",
                    'entities': [e['value'] for e in entities],
                    'context_length': len(content),
                    'word_count': len(content.split()),
                })

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ QA —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤: {len(self.qa_triplets)}")

    def _generate_targeted_answer(self, context: str, question: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower()

        # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –¥–∞—Ç–∞—Ö
        if any(word in question_lower for word in ['–∫–æ–≥–¥–∞', '–¥–∞—Ç–∞', '–≥–æ–¥', '—Å—Ä–æ–∫']):
            dates = re.findall(r'\b\d{1,2}\.\d{1,2}\.\d{4}\b', context)
            if dates:
                return f"–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –¥–∞—Ç—ã: {', '.join(dates)}. {context[:300]}..."

        # –î–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if any(word in question_lower for word in ['–∫–∞–ø–∏—Ç–∞–ª', '–∞–∫—Ü–∏', '—Ñ–∏–Ω–∞–Ω—Å', '—Ä–∞–∑–º–µ—Ä']):
            financials = re.findall(
                r'\b\d+(?:\s*\d{3})*\s*(?:–∞–∫—Ü–∏–π|—Ä—É–±–ª–µ–π|–º–ª–Ω|–º–ª—Ä–¥|—Ç–æ–Ω–Ω|–∫–º)\b',
                context
            )
            if financials:
                return f"–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏: {', '.join(financials)}. {context[:300]}..."

        # –î–ª—è –ø—Ä–æ–µ–∫—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if any(word in question_lower for word in ['–ø—Ä–æ–µ–∫—Ç', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥', '–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–≤–æ–¥']):
            projects = re.findall(
                r'(?:–ø—Ä–æ–µ–∫—Ç|–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–≤–æ–¥|—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥)\s+[¬´"]([^¬ª"]+)[¬ª"]',
                context
            )
            if projects:
                return f"–£–ø–æ–º—è–Ω—É—Ç—ã–µ –ø—Ä–æ–µ–∫—Ç—ã: {', '.join(projects)}. {context[:300]}..."

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç—Ä—ã–≤–æ–∫
        sentences = re.split(r'[.!?]+', context)
        if len(sentences) > 2:
            return sentences[0] + ". " + sentences[1] + "."
        else:
            return context[:400] + "..." if len(context) > 400 else context

    def parse(self) -> pd.DataFrame:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ DOCX –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å...")

        if not self.load_document():
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")

        self.extract_structure()
        self.create_qa_triplets()

        if not self.qa_triplets:
            logger.warning("–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ QA —Ç—Ä–∏–ø–ª–µ—Ç–∞!")
            return pd.DataFrame()

        df = pd.DataFrame(self.qa_triplets)

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        df['source_file'] = self.docx_path
        df['parse_timestamp'] = datetime.now().isoformat()

        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–∑–¥–∞–Ω–æ {len(df)} QA –ø–∞—Ä")
        return df

    def convert_to_serializable(self, obj):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ"""
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (pd.Timestamp, datetime)):
            return obj.isoformat()
        elif isinstance(obj, (pd.Series, pd.DataFrame)):
            return obj.to_dict()
        elif isinstance(obj, dict):
            return {key: self.convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self.convert_to_serializable(item) for item in obj]
        elif hasattr(obj, 'dtype'):  # numpy/pandas —Ç–∏–ø—ã
            if 'int' in str(obj.dtype):
                return int(obj)
            elif 'float' in str(obj.dtype):
                return float(obj)
            else:
                return str(obj)
        else:
            return str(obj)

    def save_benchmark(self, output_dir: str = "full_benchmark"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        Path(output_dir).mkdir(exist_ok=True)

        df = self.parse()

        if df.empty:
            logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
            return

        # 1. CSV –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        df.to_csv(f"{output_dir}/qa_pairs.csv", index=False, encoding='utf-8')

        # 2. JSON –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã (—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤)
        benchmark = {
            "metadata": {
                "company": "–ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å",
                "source_file": self.docx_path,
                "total_qa_pairs": int(len(df)),
                "total_sections": int(len(self.sections)),
                "avg_context_length": float(df['context_length'].mean()),
                "total_words": int(df['word_count'].sum()),
                "parse_date": datetime.now().isoformat()
            },
            "sections": list(self.sections.keys()),
            "qa_pairs": self.convert_to_serializable(df.to_dict('records'))
        }

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å
        benchmark_serializable = self.convert_to_serializable(benchmark)

        with open(f"{output_dir}/transneft_benchmark.json", 'w', encoding='utf-8') as f:
            json.dump(benchmark_serializable, f, ensure_ascii=False, indent=2)

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏
        with open(f"{output_dir}/contexts.txt", 'w', encoding='utf-8') as f:
            for _, row in df.iterrows():
                f.write(f"–†–ê–ó–î–ï–õ: {row['section']}\n")
                f.write(f"–ö–û–ù–¢–ï–ö–°–¢: {row['context']}\n")
                f.write(f"–í–û–ü–†–û–°: {row['question']}\n")
                f.write(f"–û–¢–í–ï–¢: {row['answer']}\n")
                f.write("=" * 100 + "\n\n")

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = self._calculate_statistics(df)
        stats_serializable = self.convert_to_serializable(stats)

        with open(f"{output_dir}/statistics.json", 'w', encoding='utf-8') as f:
            json.dump(stats_serializable, f, ensure_ascii=False, indent=2)

        self._print_statistics(stats, df)

        return df

    def _calculate_statistics(self, df: pd.DataFrame) -> Dict:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if df.empty:
            return {}

        return {
            "total_qa_pairs": int(len(df)),
            "total_sections": int(len(self.sections)),
            "avg_context_length": float(df['context_length'].mean()),
            "avg_word_count": float(df['word_count'].mean()),
            "total_words": int(df['word_count'].sum()),
            "section_distribution": {
                str(section): int(len([t for t in self.qa_triplets if t['section'] == section]))
                for section in self.sections.keys()
            }
        }

    def _print_statistics(self, stats: Dict, df: pd.DataFrame):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "=" * 70)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê DOCX –î–û–ö–£–ú–ï–ù–¢–ê –ü–ê–û –¢–†–ê–ù–°–ù–ï–§–¢–¨")
        print("=" * 70)
        print(f"üìä –í—Å–µ–≥–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {stats['total_sections']}")
        print(f"‚ùì –í—Å–µ–≥–æ QA –ø–∞—Ä: {stats['total_qa_pairs']}")
        print(f"üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {stats['avg_context_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üî¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: {stats['total_words']}")

        print(f"\nüìÇ –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
        for i, section in enumerate(list(self.sections.keys())[:8]):
            count = stats['section_distribution'].get(str(section), 0)
            print(f"   {i + 1}. {section} ({count} QA –ø–∞—Ä)")

        if self.qa_triplets:
            print(f"\nüß™ –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
            for i, triplet in enumerate(self.qa_triplets[:3]):
                print(f"   {i + 1}. {triplet['question']}")


# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã
def safe_docx_parser(docx_path: str, output_dir: str = "safe_benchmark"):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    import pandas as pd
    import json

    print(f"üõ°Ô∏è  –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {docx_path}")

    try:
        doc = Document(docx_path)
        qa_pairs = []

        current_section = "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        current_content = []

        for i, para in enumerate(doc.paragraphs):
            text = para.text.strip()
            if not text:
                continue

            # –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            if (len(text) < 100 and
                    not text.endswith(('.', '!', '?')) and
                    (text.isupper() or para.style.name.startswith('Heading') if hasattr(para, 'style') else False)):

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–¥–µ–ª
                if current_content:
                    content_text = ' '.join(current_content)
                    if len(content_text) > 100:
                        # –°–æ–∑–¥–∞–µ–º QA –ø–∞—Ä—ã
                        questions = [
                            f"–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{current_section}'?",
                            f"–û —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{current_section}'?",
                        ]

                        for q in questions:
                            qa_pairs.append({
                                'section': current_section,
                                'context': content_text,
                                'question': q,
                                'answer': content_text[:500] + '...' if len(content_text) > 500 else content_text
                            })

                current_section = text
                current_content = []
            else:
                if len(text) > 10:
                    current_content.append(text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_content:
            content_text = ' '.join(current_content)
            if len(content_text) > 100:
                questions = [
                    f"–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{current_section}'?",
                    f"–û —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ '{current_section}'?",
                ]

                for q in questions:
                    qa_pairs.append({
                        'section': current_section,
                        'context': content_text,
                        'question': q,
                        'answer': content_text[:500] + '...' if len(content_text) > 500 else content_text
                    })

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        Path(output_dir).mkdir(exist_ok=True)
        df = pd.DataFrame(qa_pairs)

        # CSV
        df.to_csv(f"{output_dir}/qa_pairs.csv", index=False, encoding='utf-8')

        # JSON —Å —è–≤–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤
        benchmark = {
            "metadata": {
                "company": "–ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å",
                "source_file": docx_path,
                "total_qa_pairs": int(len(df)),
                "parse_date": datetime.now().isoformat()
            },
            "qa_pairs": []
        }

        for _, row in df.iterrows():
            benchmark["qa_pairs"].append({
                "section": str(row['section']),
                "context": str(row['context']),
                "question": str(row['question']),
                "answer": str(row['answer'])
            })

        with open(f"{output_dir}/transneft_benchmark.json", 'w', encoding='utf-8') as f:
            json.dump(benchmark, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –£–°–ü–ï–•! –°–æ–∑–¥–∞–Ω–æ {len(df)} QA –ø–∞—Ä!")
        print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {output_dir}")

        return df

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    docx_file = "–†–µ–µ—Å—Ç—Ä –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–º–ø–∞–Ω–∏–∏ –ü–ê–û –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞ –≤–µ—Å–Ω–∞-–ª–µ—Ç–∞ 2026.docx"

    print("–í—ã–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    print("1. –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞)")
    print("2. –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)")

    choice = input("–í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2: ").strip()

    if choice == "1":
        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        safe_docx_parser(docx_file, "safe_output")
    else:
        # –ü–æ–ª–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        parser = TransneftDocxParser(docx_file)
        df = parser.save_benchmark("full_benchmark")

        print("\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
        print("   - full_benchmark/qa_pairs.csv")
        print("   - full_benchmark/transneft_benchmark.json")
        print("   - full_benchmark/contexts.txt")
        print("   - full_benchmark/statistics.json")
