import os
import sys
import json
from typing import List, Dict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from utils.config import MAX_CHUNK_SIZE, MIN_CHUNK_SIZE, MAX_WORDS_PER_CHUNK, CHUNKS_PATH


class SemanticChunker:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤"""

    def __init__(self):
        self.max_chunk_size = MAX_CHUNK_SIZE
        self.min_chunk_size = MIN_CHUNK_SIZE
        self.max_words = MAX_WORDS_PER_CHUNK

    def create_chunks(self, elements: List[Dict]) -> List[Dict]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ chunks –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        print("‚úÇÔ∏è  –ù–∞—á–∞–ª–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è...")

        chunks = []
        current_chunk = []
        current_word_count = 0
        current_section = "–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"

        for element in elements:
            text = element['text']
            words = text.split()
            word_count = len(words)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            if self._should_start_new_chunk(element, current_chunk, current_word_count):
                if current_chunk and current_word_count >= self.min_chunk_size:
                    chunks.append(self._create_chunk(current_chunk, len(chunks)))

                current_chunk = [element]
                current_word_count = word_count
                current_section = element.get('section', current_section)
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏ –ª–∏–º–∏—Ç
            if current_word_count + word_count > self.max_words and current_chunk:
                chunks.append(self._create_chunk(current_chunk, len(chunks)))
                current_chunk = [element]
                current_word_count = word_count
            else:
                current_chunk.append(element)
                current_word_count += word_count

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π chunk
        if current_chunk and current_word_count >= self.min_chunk_size:
            chunks.append(self._create_chunk(current_chunk, len(chunks)))

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö chunks")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks
        self._save_chunks(chunks)

        return chunks

    def _should_start_new_chunk(self, element: Dict, current_chunk: List, current_word_count: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∏–Ω–∞—Ç—å –Ω–æ–≤—ã–π chunk"""
        element_type = element['type']

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤ –≤—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞—é—Ç –Ω–æ–≤—ã–µ chunks
        if element_type == "section_header":
            return True

        # –ì–æ–¥—ã –Ω–∞—á–∏–Ω–∞—é—Ç –Ω–æ–≤—ã–µ chunks –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π chunk –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
        if element_type == "year_header" and current_word_count >= self.min_chunk_size:
            return True

        # –ü—Ä–æ–µ–∫—Ç—ã –Ω–∞—á–∏–Ω–∞—é—Ç –Ω–æ–≤—ã–µ chunks
        if element_type == "project_header" and current_word_count >= self.min_chunk_size:
            return True

        return False

    def _create_chunk(self, elements: List[Dict], chunk_id: int) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç chunk –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        chunk_text = "\n".join([elem['text'] for elem in elements])

        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        sections = list(set([elem['section'] for elem in elements]))
        element_types = list(set([elem['type'] for elem in elements]))
        word_count = len(chunk_text.split())

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        is_structured = any(elem['type'] in ['numbered_item', 'bullet_item'] for elem in elements)

        metadata = {
            'chunk_id': chunk_id,
            'sections': sections,
            'element_types': element_types,
            'num_elements': len(elements),
            'word_count': word_count,
            'is_structured': is_structured,
            'has_header': any(elem['type'] == 'section_header' for elem in elements)
        }

        return {
            'text': chunk_text,
            'metadata': metadata,
            'elements': [elem['element_id'] for elem in elements]
        }

    def _save_chunks(self, chunks: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç chunks –≤ —Ñ–∞–π–ª"""
        try:
            with open(CHUNKS_PATH, 'w', encoding='utf-8') as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            print(f"üíæ Chunks —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {CHUNKS_PATH}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è chunks: {e}")

    def analyze_chunks(self, chunks: List[Dict]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ chunks"""
        print("\nüìä –ê–ù–ê–õ–ò–ó CHUNKS:")
        print("-" * 40)

        total_chunks = len(chunks)
        total_words = sum(chunk['metadata']['word_count'] for chunk in chunks)
        structured_chunks = sum(1 for chunk in chunks if chunk['metadata']['is_structured'])

        print(f"üì¶ –í—Å–µ–≥–æ chunks: {total_chunks}")
        print(f"üìù –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
        print(f"üìã –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {total_words / total_chunks:.1f} —Å–ª–æ–≤/chunk")
        print(f"üèóÔ∏è  –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö chunks: {structured_chunks} ({structured_chunks / total_chunks:.1%})")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
        size_ranges = {'small': 0, 'medium': 0, 'large': 0}
        for chunk in chunks:
            words = chunk['metadata']['word_count']
            if words < 100:
                size_ranges['small'] += 1
            elif words < 250:
                size_ranges['medium'] += 1
            else:
                size_ranges['large'] += 1

        print("\nüìè –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º:")
        for size, count in size_ranges.items():
            percentage = (count / total_chunks) * 100
            print(f"   - {size}: {count} chunks ({percentage:.1f}%)")


if __name__ == "__main__":
    from utils.config import ELEMENTS_PATH

    chunker = SemanticChunker()

    with open(ELEMENTS_PATH, 'r', encoding='utf-8') as f:
        elements = json.load(f)

    chunks = chunker.create_chunks(elements)
    chunker.analyze_chunks(chunks)