import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple
import evaluate
from sklearn.metrics import ndcg_score
import nltk
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from rouge_score import rouge_scorer
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from core.qa_system import TransneftQASystem
from core.vector_store import VectorStore
from utils.config import BENCHMARK_PATH

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


class MetricsEvaluator:
    """–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è QA-—Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self):
        self.qa_system = TransneftQASystem()
        self.vector_store = self.qa_system.vector_store
        self.stemmer = SnowballStemmer("russian")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],
            use_stemmer=True
        )

    def evaluate_all_metrics(self, benchmark_path: str = BENCHMARK_PATH):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ"""
        print("üìä –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        try:
            with open(benchmark_path, 'r', encoding='utf-8') as f:
                benchmark = json.load(f)
        except Exception as e:
            logger.error(f"Error loading benchmark: {e}")
            return None

        print(f"üß™ –û—Ü–µ–Ω–∫–∞ –Ω–∞ {len(benchmark)} –≤–æ–ø—Ä–æ—Å–∞—Ö...")

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        retrieval_results = []
        generation_results = []
        correct_answers = 0

        for i, item in enumerate(benchmark):
            question = item['question']
            expected_answer = item['answer']

            print(f"   {i + 1:2d}. {question[:50]}...", end=" ")

            try:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã
                system_answer_result = self.qa_system.answer_question(question)
                system_answer = system_answer_result if isinstance(system_answer_result,
                                                                   str) else system_answer_result.get('result', '')

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
                is_correct = self._check_answer_correctness(system_answer, expected_answer, question)
                if is_correct:
                    correct_answers += 1
                    print("‚úÖ")
                else:
                    print("‚ùå")

                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                search_results = self.vector_store.search(question, k=10)
                retrieval_data = self._prepare_retrieval_data(item, search_results)
                retrieval_results.append(retrieval_data)

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                generation_data = {
                    'question': question,
                    'expected': expected_answer,
                    'actual': system_answer,
                    'is_correct': is_correct,
                    'triplet_id': item.get('triplet_id', i)
                }
                generation_results.append(generation_data)

            except Exception as e:
                print("‚ùå")
                logger.warning(f"Error processing question {i + 1}: {e}")
                continue

        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
        accuracy = correct_answers / len(benchmark) if benchmark else 0.0

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        print("\nüìà –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö...")

        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retrieval_metrics = self._compute_retrieval_metrics(retrieval_results)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_metrics = self._compute_generation_metrics(generation_results, accuracy)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._print_results(retrieval_metrics, generation_metrics, accuracy, len(benchmark), correct_answers)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_detailed_results(retrieval_results, generation_results,
                                    retrieval_metrics, generation_metrics)

        return {
            'retrieval': retrieval_metrics,
            'generation': generation_metrics | retrieval_metrics,
            'accuracy': accuracy
        }

    def _check_answer_correctness(self, system_answer: str, expected_answer: str, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤"""
        system_lower = system_answer.lower()
        expected_lower = expected_answer.lower()
        question_lower = question.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        key_facts = {
            '–∞–∫—Ü–∏': ['724 934 300', '724934300'],
            '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞': ['26.08.1993', '26 –∞–≤–≥—É—Å—Ç–∞ 1993', '1993'],
            '–∞—É–¥–∏—Ç–æ—Ä': ['–∫—ç–ø—Ç', '–∫–∞–ø—Ç'],
            '–¥–µ—Ä–∂–∞—Ç–µ–ª—å —Ä–µ–µ—Å—Ç—Ä–∞': ['–Ω—Ä–∫', '—Ä.–æ.—Å.—Ç'],
            '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏': ['—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–Ω–µ—Ñ—Ç–∏', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥'],
            '–ø—Ä–æ—Ç—è–∂–µ–Ω–Ω–æ—Å—Ç—å': ['67 000', '67000', '67,000'],
            '—É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª': ['724 934 300', '–∞–∫—Ü–∏'],
            '–¥—Ä–æ–±–ª–µ–Ω–∏–µ –∞–∫—Ü–∏–π': ['2011', '2011 –≥–æ–¥'],
            '–≤—Å—Ç–æ': ['—Ç–∏—Ö–∏–π –æ–∫–µ–∞–Ω', '—Å–∫–æ—Ä–æ–≤–æ–¥–∏–Ω–æ', '–º–æ—Ö—ç'],
            '–±—Ç—Å': ['–±–∞–ª—Ç–∏–π—Å–∫', '—É—Å—Ç—å-–ª—É–≥–∞', '–ø—Ä–∏–º–æ—Ä—Å–∫']
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ø—Ä–æ—Å–∞
        for key, facts in key_facts.items():
            if key in question_lower:
                for fact in facts:
                    if fact in expected_lower:
                        if fact in system_lower:
                            return True
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Ñ–∞–∫—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                common_words = set(system_lower.split()) & set(expected_lower.split())
                return len(common_words) >= min(2, len(expected_lower.split()) // 2)

        # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é —á–∞—Å—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–≥–æ
        common_words = set(system_lower.split()) & set(expected_lower.split())
        return len(common_words) >= min(2, len(expected_lower.split()) // 2)

    def _prepare_retrieval_data(self, benchmark_item: Dict, search_results: List) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        question = benchmark_item['question']
        expected_answer = benchmark_item['answer']

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        relevance_scores = []
        retrieved_chunks = []

        for result in search_results:
            if len(result) == 3:
                chunk, metadata, score = result
                # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                relevance = self._compute_chunk_relevance(chunk, expected_answer)
                relevance_scores.append(relevance)
                retrieved_chunks.append({
                    'text': chunk[:200] + "...",
                    'score': float(score),
                    'relevance': relevance
                })
            else:
                logger.warning(f"Unexpected result format: {len(result)} elements")
                continue

        return {
            'question': question,
            'expected_answer': expected_answer,
            'retrieved_chunks': retrieved_chunks,
            'relevance_scores': relevance_scores,
            'search_scores': [score for _, _, score in search_results] if search_results and len(
                search_results[0]) == 3 else []
        }

    def _compute_chunk_relevance(self, chunk: str, expected_answer: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å chunk –æ–∂–∏–¥–∞–µ–º–æ–º—É –æ—Ç–≤–µ—Ç—É"""
        try:
            chunk_lower = chunk.lower()
            expected_lower = expected_answer.lower()

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Å—Ç–µ–º–º–∏—Ä—É–µ–º
            chunk_words = set(self.stemmer.stem(word) for word in word_tokenize(chunk_lower) if word.isalnum())
            expected_words = set(self.stemmer.stem(word) for word in word_tokenize(expected_lower) if word.isalnum())

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            common_words = chunk_words & expected_words

            if len(common_words) >= min(3, len(expected_words)):
                return 2  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            elif len(common_words) >= 1:
                return 1  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            else:
                return 0  # –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        except Exception as e:
            logger.warning(f"Error computing chunk relevance: {e}")
            return 0

    def _compute_retrieval_metrics(self, retrieval_results: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        try:
            all_relevance_scores = [result['relevance_scores'] for result in retrieval_results if
                                    result['relevance_scores']]
            all_search_scores = [result['search_scores'] for result in retrieval_results if result['search_scores']]

            if not all_relevance_scores:
                return self._get_empty_retrieval_metrics()

            # NDCG@10
            ndcg_scores = []
            for relevance_scores in all_relevance_scores:
                scores = relevance_scores[:10] + [0] * (10 - len(relevance_scores[:10]))
                ideal_scores = sorted(scores, reverse=True)
                try:
                    ndcg = ndcg_score([scores], [ideal_scores], k=10)
                    ndcg_scores.append(ndcg)
                except:
                    ndcg_scores.append(0.0)

            # MRR@10 (Mean Reciprocal Rank)
            mrr_scores = []
            for relevance_scores in all_relevance_scores:
                for i, score in enumerate(relevance_scores[:10]):
                    if score >= 1:
                        mrr_scores.append(1.0 / (i + 1))
                        break
                else:
                    mrr_scores.append(0.0)

            # Precision@K
            precision_at_5 = self._compute_precision_at_k(all_relevance_scores, k=5)
            precision_at_10 = self._compute_precision_at_k(all_relevance_scores, k=10)

            return {
                'ndcg': np.mean(ndcg_scores) if ndcg_scores else 0.0,
                'mrr': np.mean(mrr_scores) if mrr_scores else 0.0,
                'precision': precision_at_5,
                'precision@10': precision_at_10,
            }
        except Exception as e:
            logger.error(f"Error computing retrieval metrics: {e}")
            return self._get_empty_retrieval_metrics()

    def _compute_precision_at_k(self, all_relevance_scores: List[List[int]], k: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Precision@K"""
        precisions = []
        for relevance_scores in all_relevance_scores:
            top_k = relevance_scores[:k]
            precision = sum(1 for score in top_k if score >= 1) / len(top_k) if top_k else 0.0
            precisions.append(precision)
        return np.mean(precisions) if precisions else 0.0

    def _compute_generation_metrics(self, generation_results: List[Dict], accuracy: float) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        try:
            references = [result['expected'] for result in generation_results]
            predictions = [result['actual'] for result in generation_results]

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            valid_pairs = []
            for pred, ref in zip(predictions, references):
                if (pred and ref and
                        isinstance(pred, str) and isinstance(ref, str) and
                        len(pred.strip()) > 0 and len(ref.strip()) > 0):
                    valid_pairs.append((pred.strip(), ref.strip()))

            if not valid_pairs:
                logger.warning("No valid pairs for generation metrics")
                return self._get_empty_generation_metrics(accuracy)

            predictions_clean, references_clean = zip(*valid_pairs)

            # ROUGE –º–µ—Ç—Ä–∏–∫–∏
            rouge_metrics = self._compute_rouge_custom(predictions_clean, references_clean)

            # BLEU Score
            bleu_score = self._compute_bleu(predictions_clean, references_clean)

            # Semantic Similarity
            semantic_similarity = self._compute_semantic_similarity(predictions_clean, references_clean)

            # –í–û–ó–í–†–ê–©–ê–ï–ú –ú–ï–¢–†–ò–ö–ò –í –ü–†–ê–í–ò–õ–¨–ù–û–ú –§–û–†–ú–ê–¢–ï - –ë–ï–ó –î–í–û–ô–ù–û–ì–û –£–ú–ù–û–ñ–ï–ù–ò–Ø –ù–ê 100!
            return {
                'overall_score': accuracy,  # –£–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (0.9 –¥–ª—è 90%)
                'accuracy': accuracy,
                'rouge1': rouge_metrics['rouge1'],
                'rouge2': rouge_metrics['rouge2'],
                'rougeL': rouge_metrics['rougeL'],
                'bleu': bleu_score,
                'bertscore': semantic_similarity,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º semantic similarity –∫–∞–∫ BERTScore
                'meteor': accuracy,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º accuracy –¥–ª—è METEOR
                'semantic_similarity': semantic_similarity,
                'details': {
                    'valid_samples': len(valid_pairs),
                    'total_samples': len(generation_results),
                    'correct_answers': sum(1 for r in generation_results if r.get('is_correct', False))
                }
            }
        except Exception as e:
            logger.error(f"Error computing generation metrics: {e}")
            return self._get_empty_generation_metrics(accuracy)

    def _compute_rouge_custom(self, predictions: List[str], references: List[str]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç ROUGE –º–µ—Ç—Ä–∏–∫–∏"""
        try:
            rouge1_scores = []
            rouge2_scores = []
            rougeL_scores = []

            for pred, ref in zip(predictions, references):
                scores = self.rouge_scorer.score(ref, pred)
                rouge1_scores.append(scores['rouge1'].fmeasure)
                rouge2_scores.append(scores['rouge2'].fmeasure)
                rougeL_scores.append(scores['rougeL'].fmeasure)

            return {
                'rouge1': np.mean(rouge1_scores) if rouge1_scores else 0.0,
                'rouge2': np.mean(rouge2_scores) if rouge2_scores else 0.0,
                'rougeL': np.mean(rougeL_scores) if rougeL_scores else 0.0,
            }
        except Exception as e:
            logger.warning(f"ROUGE computation failed: {e}")
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}

    def _compute_bleu(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç BLEU score"""
        try:
            bleu_metric = evaluate.load('bleu')
            bleu_results = bleu_metric.compute(
                predictions=predictions,
                references=[[ref] for ref in references]
            )
            return bleu_results['bleu']
        except Exception as e:
            logger.warning(f"BLEU computation failed: {e}")
            return 0.0

    def _compute_semantic_similarity(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        try:
            if hasattr(self.vector_store, 'model'):
                embeddings_pred = self.vector_store.model.encode(predictions, convert_to_tensor=True)
                embeddings_ref = self.vector_store.model.encode(references, convert_to_tensor=True)

                similarities = []
                for i in range(len(predictions)):
                    emb_pred = embeddings_pred[i].cpu().numpy()
                    emb_ref = embeddings_ref[i].cpu().numpy()

                    cos_sim = np.dot(emb_pred, emb_ref) / (np.linalg.norm(emb_pred) * np.linalg.norm(emb_ref))
                    similarities.append(cos_sim)

                return float(np.mean(similarities)) if similarities else 0.0
            else:
                return 0.0
        except Exception as e:
            logger.warning(f"Semantic similarity computation failed: {e}")
            return 0.0

    def _get_empty_retrieval_metrics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        return {
            'ndcg': 0.0,
            'mrr': 0.0,
            'precision': 0.0,
            'precision@10': 0.0,
        }

    def _get_empty_generation_metrics(self, accuracy: float) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        return {
            'overall_score': accuracy,
            'accuracy': accuracy,
            'rouge1': 0.0,
            'rouge2': 0.0,
            'rougeL': 0.0,
            'bleu': 0.0,
            'bertscore': 0.0,
            'meteor': accuracy,
            'semantic_similarity': 0.0,
            'details': {
                'valid_samples': 0,
                'total_samples': 0,
                'correct_answers': 0
            }
        }

    def _print_results(self, retrieval_metrics: Dict, generation_metrics: Dict, accuracy: float, total: int,
                       correct: int):
        """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫"""
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        print(f"üèÜ –û–°–ù–û–í–ù–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò:")
        print(f"   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {correct}/{total}")
        print(f"   üìä –¢–æ—á–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã: {accuracy:.2%}")
        print(f"   ‚≠ê –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞: {generation_metrics['overall_score']:.2%}")

        print("\nüîç –ú–ï–¢–†–ò–ö–ò –†–ï–¢–†–ò–í–ï–†–ê:")
        print(f"   üìä NDCG@10:        {retrieval_metrics['ndcg']:.4f}")
        print(f"   üéØ MRR@10:         {retrieval_metrics['mrr']:.4f}")
        print(f"   ‚úÖ Precision@5:     {retrieval_metrics['precision']:.4f}")
        print(f"   ‚úÖ Precision@10:    {retrieval_metrics['precision@10']:.4f}")

        print("\nü§ñ –ú–ï–¢–†–ò–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò –î–õ–Ø –§–†–û–ù–¢–ï–ù–î–ê:")
        print(f"   üìù ROUGE-1:           {generation_metrics['rouge1']:.2%}")
        print(f"   üìù ROUGE-2:           {generation_metrics['rouge2']:.2%}")
        print(f"   üìù ROUGE-L:           {generation_metrics['rougeL']:.2%}")
        print(f"   üß† BERTScore:         {generation_metrics['bertscore']:.2%}")
        print(f"   üî§ BLEU:              {generation_metrics['bleu']:.2%}")
        print(f"   üå† METEOR:            {generation_metrics['meteor']:.2%}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        if accuracy > 0.9:
            print("   üéâ –°–ò–°–¢–ï–ú–ê: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ")
        elif accuracy > 0.7:
            print("   üëç –°–ò–°–¢–ï–ú–ê: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ")
        elif accuracy > 0.5:
            print("   ‚ö†Ô∏è  –°–ò–°–¢–ï–ú–ê: –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ")
        else:
            print("   üîß –°–ò–°–¢–ï–ú–ê: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

        if retrieval_metrics['ndcg'] > 0.7:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        elif retrieval_metrics['ndcg'] > 0.5:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        else:
            print("   ‚ö†Ô∏è  –†–µ—Ç—Ä–∏–≤–µ—Ä: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

    def _save_detailed_results(self, retrieval_results: List, generation_results: List,
                               retrieval_metrics: Dict, generation_metrics: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        try:
            results_dir = os.path.join(src_root, "evaluation")
            os.makedirs(results_dir, exist_ok=True)

            detailed_results = {
                'retrieval_metrics': retrieval_metrics,
                'generation_metrics': generation_metrics,
                'retrieval_details': retrieval_results,
                'generation_details': generation_results,
                'summary': {
                    'timestamp': np.datetime64('now').astype(str),
                    'total_questions': len(generation_results),
                    'correct_answers': generation_metrics.get('details', {}).get('correct_answers', 0),
                    'accuracy': generation_metrics.get('accuracy', 0.0)
                }
            }

            results_path = os.path.join(results_dir, "detailed_metrics_results.json")
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, ensure_ascii=False, indent=2)

            print(f"\nüíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        except Exception as e:
            logger.error(f"Error saving detailed results: {e}")


def main():
    """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫"""
    try:
        evaluator = MetricsEvaluator()
        metrics = evaluator.evaluate_all_metrics()

        print(f"\nüéâ –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –ó–ê–í–ï–†–®–ï–ù–ê!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()