import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple
import evaluate
from sklearn.metrics import ndcg_score
import nltk
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from core.qa_system import TransneftQASystem
from core.vector_store import VectorStore
from utils.config import BENCHMARK_PATH

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


class MetricsEvaluator:
    """–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è QA-—Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self):
        self.qa_system = TransneftQASystem()
        self.vector_store = self.qa_system.vector_store
        self.stemmer = SnowballStemmer("russian")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.bleurt = evaluate.load("bleurt", module_type="metric")
        self.rouge = evaluate.load("rouge")

    def evaluate_all_metrics(self, benchmark_path: str = BENCHMARK_PATH):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ"""
        print("üìä –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        with open(benchmark_path, 'r', encoding='utf-8') as f:
            benchmark = json.load(f)

        print(f"üß™ –û—Ü–µ–Ω–∫–∞ –Ω–∞ {len(benchmark)} –≤–æ–ø—Ä–æ—Å–∞—Ö...")

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        retrieval_results = []
        generation_results = []

        for i, item in enumerate(benchmark):
            question = item['question']
            expected_answer = item['answer']

            print(f"   {i + 1:2d}. {question[:50]}...")

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã
            system_answer = self.qa_system.answer_question(question)

            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            search_results = self.vector_store.search(question, k=10)
            retrieval_data = self._prepare_retrieval_data(item, search_results)
            retrieval_results.append(retrieval_data)

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            generation_data = {
                'question': question,
                'expected': expected_answer,
                'actual': system_answer
            }
            generation_results.append(generation_data)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        print("\nüìà –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö...")

        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retrieval_metrics = self._compute_retrieval_metrics(retrieval_results)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_metrics = self._compute_generation_metrics(generation_results)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._print_results(retrieval_metrics, generation_metrics)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_detailed_results(retrieval_results, generation_results,
                                    retrieval_metrics, generation_metrics)

        return {
            'retrieval': retrieval_metrics,
            'generation': generation_metrics
        }

    def _prepare_retrieval_data(self, benchmark_item: Dict, search_results: List) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        question = benchmark_item['question']
        expected_answer = benchmark_item['answer']

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        relevance_scores = []
        retrieved_chunks = []

        for chunk, metadata, score in search_results:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            relevance = self._compute_chunk_relevance(chunk, expected_answer)
            relevance_scores.append(relevance)
            retrieved_chunks.append({
                'text': chunk[:200] + "...",  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–≤—å—é
                'score': float(score),
                'relevance': relevance
            })

        return {
            'question': question,
            'expected_answer': expected_answer,
            'retrieved_chunks': retrieved_chunks,
            'relevance_scores': relevance_scores,
            'search_scores': [score for _, _, score in search_results]
        }

    def _compute_chunk_relevance(self, chunk: str, expected_answer: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å chunk –æ–∂–∏–¥–∞–µ–º–æ–º—É –æ—Ç–≤–µ—Ç—É"""
        chunk_lower = chunk.lower()
        expected_lower = expected_answer.lower()

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Å—Ç–µ–º–º–∏—Ä—É–µ–º
        chunk_words = set(self.stemmer.stem(word) for word in word_tokenize(chunk_lower) if word.isalnum())
        expected_words = set(self.stemmer.stem(word) for word in word_tokenize(expected_lower) if word.isalnum())

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        common_words = chunk_words & expected_words

        if len(common_words) >= min(3, len(expected_words)):
            return 2  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        elif len(common_words) >= 1:
            return 1  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        else:
            return 0  # –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å

    def _compute_retrieval_metrics(self, retrieval_results: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        all_relevance_scores = [result['relevance_scores'] for result in retrieval_results]
        all_search_scores = [result['search_scores'] for result in retrieval_results]

        # NDCG@10
        ndcg_scores = []
        for relevance_scores in all_relevance_scores:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            scores = relevance_scores[:10] + [0] * (10 - len(relevance_scores[:10]))
            ideal_scores = sorted(scores, reverse=True)
            ndcg = ndcg_score([scores], [ideal_scores], k=10)
            ndcg_scores.append(ndcg)

        # MRR@10 (Mean Reciprocal Rank)
        mrr_scores = []
        for relevance_scores in all_relevance_scores:
            for i, score in enumerate(relevance_scores[:10]):
                if score >= 1:  # –ù–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    mrr_scores.append(1.0 / (i + 1))
                    break
            else:
                mrr_scores.append(0.0)

        # MAP@100 (Mean Average Precision)
        map_scores = []
        for relevance_scores in all_relevance_scores:
            precision_scores = []
            relevant_count = 0
            for i, score in enumerate(relevance_scores[:100]):
                if score >= 1:
                    relevant_count += 1
                    precision_scores.append(relevant_count / (i + 1))
            if precision_scores:
                map_scores.append(sum(precision_scores) / len(precision_scores))
            else:
                map_scores.append(0.0)

        return {
            'ndcg@10': np.mean(ndcg_scores),
            'mrr@10': np.mean(mrr_scores),
            'map@100': np.mean(map_scores),
            'details': {
                'ndcg_scores': ndcg_scores,
                'mrr_scores': mrr_scores,
                'map_scores': map_scores
            }
        }

    def _compute_generation_metrics(self, generation_results: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        references = [result['expected'] for result in generation_results]
        predictions = [result['actual'] for result in generation_results]

        # BLEURT-20
        bleurt_scores = self.bleurt.compute(
            predictions=predictions,
            references=references
        )['scores']

        # ROUGE —Å —Å—Ç–µ–º–º–∏–Ω–≥–æ–º
        rouge_results = self.rouge.compute(
            predictions=predictions,
            references=references,
            use_stemmer=True
        )

        # Semantic Answer Similarity (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        semantic_similarity = self._compute_semantic_similarity(predictions, references)

        return {
            'bleurt': np.mean(bleurt_scores),
            'rouge1': rouge_results['rouge1'],
            'rouge2': rouge_results['rouge2'],
            'rougeL': rouge_results['rougeL'],
            'semantic_similarity': semantic_similarity,
            'details': {
                'bleurt_scores': bleurt_scores,
                'rouge_scores': rouge_results
            }
        }

    def _compute_semantic_similarity(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        embeddings_pred = self.vector_store.model.encode(predictions, convert_to_tensor=True)
        embeddings_ref = self.vector_store.model.encode(references, convert_to_tensor=True)

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for i in range(len(predictions)):
            cos_sim = np.dot(
                embeddings_pred[i].cpu().numpy(),
                embeddings_ref[i].cpu().numpy()
            ) / (np.linalg.norm(embeddings_pred[i].cpu().numpy()) * np.linalg.norm(embeddings_ref[i].cpu().numpy()))
            similarities.append(cos_sim)

        return float(np.mean(similarities))

    def _print_results(self, retrieval_metrics: Dict, generation_metrics: Dict):
        """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫"""
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        print("üîç –ú–ï–¢–†–ò–ö–ò –†–ï–¢–†–ò–í–ï–†–ê:")
        print(f"   üìä NDCG@10:    {retrieval_metrics['ndcg@10']:.4f}")
        print(f"   üéØ MRR@10:     {retrieval_metrics['mrr@10']:.4f}")
        print(f"   üó∫Ô∏è  MAP@100:    {retrieval_metrics['map@100']:.4f}")

        print("\nü§ñ –ú–ï–¢–†–ò–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–í–ï–¢–û–í:")
        print(f"   üíé BLEURT:            {generation_metrics['bleurt']:.4f}")
        print(f"   üìù ROUGE-1:           {generation_metrics['rouge1']:.4f}")
        print(f"   üìù ROUGE-2:           {generation_metrics['rouge2']:.4f}")
        print(f"   üìù ROUGE-L:           {generation_metrics['rougeL']:.4f}")
        print(f"   üß† Semantic Similarity: {generation_metrics['semantic_similarity']:.4f}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        if retrieval_metrics['ndcg@10'] > 0.7:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        elif retrieval_metrics['ndcg@10'] > 0.5:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        else:
            print("   ‚ö†Ô∏è  –†–µ—Ç—Ä–∏–≤–µ—Ä: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

        if generation_metrics['rougeL'] > 0.7:
            print("   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
        elif generation_metrics['rougeL'] > 0.5:
            print("   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
        else:
            print("   ‚ö†Ô∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

    def _save_detailed_results(self, retrieval_results: List, generation_results: List,
                               retrieval_metrics: Dict, generation_metrics: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        results_dir = os.path.join(src_root, "evaluation")
        os.makedirs(results_dir, exist_ok=True)

        detailed_results = {
            'retrieval_metrics': retrieval_metrics,
            'generation_metrics': generation_metrics,
            'retrieval_details': retrieval_results,
            'generation_details': generation_results,
            'summary': {
                'timestamp': np.datetime64('now').astype(str),
                'total_questions': len(generation_results)
            }
        }

        results_path = os.path.join(results_dir, "detailed_metrics_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")


def main():
    """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫"""
    try:
        evaluator = MetricsEvaluator()
        metrics = evaluator.evaluate_all_metrics()

        print(f"\nüéâ –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –ó–ê–í–ï–†–®–ï–ù–ê!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()