import os
import sys
import json
import numpy as np
from typing import List, Dict, Tuple
import evaluate
from sklearn.metrics import ndcg_score
import nltk
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from rouge_score import rouge_scorer
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from core.qa_system import TransneftQASystem
from core.vector_store import VectorStore
from utils.config import BENCHMARK_PATH

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


class MetricsEvaluator:
    """–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è QA-—Å–∏—Å—Ç–µ–º—ã –±–µ–∑ TensorFlow"""

    def __init__(self):
        self.qa_system = TransneftQASystem()
        self.vector_store = self.qa_system.vector_store
        self.stemmer = SnowballStemmer("russian")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –±–µ–∑ TensorFlow
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=True
        )
        
        # BERTScore –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        self.bert_scorer = None



    def evaluate_all_metrics(self, benchmark_path: str = BENCHMARK_PATH):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ"""
        print("üìä –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        try:
            with open(benchmark_path, 'r', encoding='utf-8') as f:
                benchmark = json.load(f)
        except Exception as e:
            logger.error(f"Error loading benchmark: {e}")
            return None

        print(f"üß™ –û—Ü–µ–Ω–∫–∞ –Ω–∞ {len(benchmark)} –≤–æ–ø—Ä–æ—Å–∞—Ö...")

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        retrieval_results = []
        generation_results = []

        for i, item in enumerate(benchmark):
            question = item['question']
            expected_answer = item['answer']

            print(f"   {i + 1:2d}. {question[:50]}...")

            try:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã
                system_answer_result = self.qa_system.answer_question(question)
                system_answer = system_answer_result if isinstance(system_answer_result, str) else system_answer_result.get('result', '')

                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                search_results = self.vector_store.search(question, k=10)
                retrieval_data = self._prepare_retrieval_data(item, search_results)
                retrieval_results.append(retrieval_data)

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                generation_data = {
                    'question': question,
                    'expected': expected_answer,
                    'actual': system_answer
                }
                generation_results.append(generation_data)
                
            except Exception as e:
                logger.warning(f"Error processing question {i+1}: {e}")
                continue

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        print("\nüìà –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö...")

        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        retrieval_metrics = self._compute_retrieval_metrics(retrieval_results)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_metrics = self._compute_generation_metrics(generation_results)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._print_results(retrieval_metrics, generation_metrics)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_detailed_results(retrieval_results, generation_results,
                                    retrieval_metrics, generation_metrics)

        return {
            'retrieval': retrieval_metrics,
            'generation': generation_metrics,
        }
        

    def _prepare_retrieval_data(self, benchmark_item: Dict, search_results: List) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        question = benchmark_item['question']
        expected_answer = benchmark_item['answer']

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        relevance_scores = []
        retrieved_chunks = []

        for chunk, metadata, score in search_results:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            relevance = self._compute_chunk_relevance(chunk, expected_answer)
            relevance_scores.append(relevance)
            retrieved_chunks.append({
                'text': chunk[:200] + "...",  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–≤—å—é
                'score': float(score),
                'relevance': relevance
            })

        return {
            'question': question,
            'expected_answer': expected_answer,
            'retrieved_chunks': retrieved_chunks,
            'relevance_scores': relevance_scores,
            'search_scores': [score for _, _, score in search_results]
        }

    def _compute_chunk_relevance(self, chunk: str, expected_answer: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å chunk –æ–∂–∏–¥–∞–µ–º–æ–º—É –æ—Ç–≤–µ—Ç—É"""
        try:
            chunk_lower = chunk.lower()
            expected_lower = expected_answer.lower()

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Å—Ç–µ–º–º–∏—Ä—É–µ–º
            chunk_words = set(self.stemmer.stem(word) for word in word_tokenize(chunk_lower) if word.isalnum())
            expected_words = set(self.stemmer.stem(word) for word in word_tokenize(expected_lower) if word.isalnum())

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            common_words = chunk_words & expected_words

            if len(common_words) >= min(3, len(expected_words)):
                return 2  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            elif len(common_words) >= 1:
                return 1  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            else:
                return 0  # –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        except Exception as e:
            logger.warning(f"Error computing chunk relevance: {e}")
            return 0

    def _compute_retrieval_metrics(self, retrieval_results: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
        try:
            all_relevance_scores = [result['relevance_scores'] for result in retrieval_results]
            all_search_scores = [result['search_scores'] for result in retrieval_results]

            # NDCG@10
            ndcg_scores = []
            for relevance_scores in all_relevance_scores:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                scores = relevance_scores[:10] + [0] * (10 - len(relevance_scores[:10]))
                ideal_scores = sorted(scores, reverse=True)
                ndcg = ndcg_score([scores], [ideal_scores], k=10)
                ndcg_scores.append(ndcg)

            # MRR@10 (Mean Reciprocal Rank)
            mrr_scores = []
            for relevance_scores in all_relevance_scores:
                for i, score in enumerate(relevance_scores[:10]):
                    if score >= 1:  # –ù–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                        mrr_scores.append(1.0 / (i + 1))
                        break
                else:
                    mrr_scores.append(0.0)

            # MAP@100 (Mean Average Precision)
            map_scores = []
            for relevance_scores in all_relevance_scores:
                precision_scores = []
                relevant_count = 0
                for i, score in enumerate(relevance_scores[:100]):
                    if score >= 1:
                        relevant_count += 1
                        precision_scores.append(relevant_count / (i + 1))
                if precision_scores:
                    map_scores.append(sum(precision_scores) / len(precision_scores))
                else:
                    map_scores.append(0.0)

            # Precision@K
            precision_at_5 = self._compute_precision_at_k(all_relevance_scores, k=5)
            precision_at_10 = self._compute_precision_at_k(all_relevance_scores, k=10)

            return {
                'ndcg@10': np.mean(ndcg_scores),
                'mrr@10': np.mean(mrr_scores),
                'map@100': np.mean(map_scores),
                'precision@5': precision_at_5,
                'precision@10': precision_at_10,
                'details': {
                    'ndcg_scores': ndcg_scores,
                    'mrr_scores': mrr_scores,
                    'map_scores': map_scores
                }
            }
        except Exception as e:
            logger.error(f"Error computing retrieval metrics: {e}")
            return {
                'ndcg@10': 0.0,
                'mrr@10': 0.0,
                'map@100': 0.0,
                'precision@5': 0.0,
                'precision@10': 0.0,
                'details': {}
            }

    def _compute_precision_at_k(self, all_relevance_scores: List[List[int]], k: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Precision@K"""
        precisions = []
        for relevance_scores in all_relevance_scores:
            top_k = relevance_scores[:k]
            precision = sum(1 for score in top_k if score >= 1) / len(top_k) if top_k else 0.0
            precisions.append(precision)
        return np.mean(precisions)

    def _compute_generation_metrics(self, generation_results: List[Dict]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–µ–∑ TensorFlow"""
        try:
            references = [result['expected'] for result in generation_results]
            predictions = [result['actual'] for result in generation_results]

            # ROUGE –±–µ–∑ TensorFlow
            rouge_metrics = self._compute_rouge_custom(predictions, references)
            
            # METEOR
            meteor_score = self._compute_meteor(predictions, references)
            
            # BERTScore
            bertscore_metrics = {'f1': 0, 'recall': 0, 'precision':0}
            
            # BLEU Score
            bleu_score = self._compute_bleu(predictions, references)
            
            # Semantic Similarity
            semantic_similarity = self._compute_semantic_similarity(predictions, references)

            res = {
                'rouge1': rouge_metrics['rouge1'],
                'rouge2': rouge_metrics['rouge2'],
                'rougeL': rouge_metrics['rougeL'],
                'meteor': meteor_score,
                'bertscore_precision': bertscore_metrics['precision'],
                'bertscore_recall': bertscore_metrics['recall'],
                'bertscore_f1': bertscore_metrics['f1'],
                'bleu': bleu_score,
                'semantic_similarity': semantic_similarity,
                'details': {
                    'rouge_scores': rouge_metrics,
                    'bertscore_scores': bertscore_metrics
                }
            }
            return res
        except Exception as e:
            logger.error(f"Error computing generation metrics: {e}")
            return self._get_empty_generation_metrics()


    def _compute_rouge_custom(self, predictions: List[str], references: List[str]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç ROUGE –º–µ—Ç—Ä–∏–∫–∏ –±–µ–∑ TensorFlow"""
        try:
            rouge1_scores = []
            rouge2_scores = []
            rougeL_scores = []
            
            for pred, ref in zip(predictions, references):
                scores = self.rouge_scorer.score(ref, pred)
                rouge1_scores.append(scores['rouge1'].fmeasure)
                rouge2_scores.append(scores['rouge2'].fmeasure)
                rougeL_scores.append(scores['rougeL'].fmeasure)
            
            return {
                'rouge1': np.mean(rouge1_scores),
                'rouge2': np.mean(rouge2_scores),
                'rougeL': np.mean(rougeL_scores),
                'rouge1_scores': rouge1_scores,
                'rouge2_scores': rouge2_scores,
                'rougeL_scores': rougeL_scores
            }
        except Exception as e:
            logger.warning(f"ROUGE computation failed: {e}")
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}

    def _compute_meteor(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç METEOR –º–µ—Ç—Ä–∏–∫—É"""
        return 0.0


    def _compute_bleu(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç BLEU score"""
        try:
            bleu_metric = evaluate.load('bleu')
            bleu_results = bleu_metric.compute(
                predictions=predictions,
                references=[[ref] for ref in references]  # BLEU —Ç—Ä–µ–±—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤
            )
            return bleu_results['bleu']
        except Exception as e:
            logger.warning(f"BLEU computation failed: {e}")
            return 0.0

    def _compute_semantic_similarity(self, predictions: List[str], references: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏ –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
            if hasattr(self.vector_store, 'model'):
                embeddings_pred = self.vector_store.model.encode(predictions, convert_to_tensor=True)
                embeddings_ref = self.vector_store.model.encode(references, convert_to_tensor=True)

                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarities = []
                for i in range(len(predictions)):
                    cos_sim = np.dot(
                        embeddings_pred[i].cpu().numpy(),
                        embeddings_ref[i].cpu().numpy()
                    ) / (np.linalg.norm(embeddings_pred[i].cpu().numpy()) * np.linalg.norm(embeddings_ref[i].cpu().numpy()))
                    similarities.append(cos_sim)

                return float(np.mean(similarities))
            else:
                return 0.0
        except Exception as e:
            logger.warning(f"Semantic similarity computation failed: {e}")
            return 0.0

    def _get_empty_generation_metrics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        return {
            'rouge1': 0.0,
            'rouge2': 0.0,
            'rougeL': 0.0,
            'meteor': 0.0,
            'bertscore_precision': 0.0,
            'bertscore_recall': 0.0,
            'bertscore_f1': 0.0,
            'bleu': 0.0,
            'semantic_similarity': 0.0,
            'details': {}
        }

    def _print_results(self, retrieval_metrics: Dict, generation_metrics: Dict):
        """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫"""
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê")
        print("=" * 60)

        print("üîç –ú–ï–¢–†–ò–ö–ò –†–ï–¢–†–ò–í–ï–†–ê:")
        print(f"   üìä NDCG@10:        {retrieval_metrics['ndcg@10']:.4f}")
        print(f"   üéØ MRR@10:         {retrieval_metrics['mrr@10']:.4f}")
        print(f"   üó∫Ô∏è  MAP@100:        {retrieval_metrics['map@100']:.4f}")
        print(f"   ‚úÖ Precision@5:     {retrieval_metrics['precision@5']:.4f}")
        print(f"   ‚úÖ Precision@10:    {retrieval_metrics['precision@10']:.4f}")

        print("\nü§ñ –ú–ï–¢–†–ò–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–í–ï–¢–û–í:")
        print(f"   üìù ROUGE-1:           {generation_metrics['rouge1']:.4f}")
        print(f"   üìù ROUGE-2:           {generation_metrics['rouge2']:.4f}")
        print(f"   üìù ROUGE-L:           {generation_metrics['rougeL']:.4f}")
        print(f"   üå† METEOR:            {generation_metrics['meteor']:.4f}")
        print(f"   üß† BERTScore F1:      {generation_metrics['bertscore_f1']:.4f}")
        print(f"   üî§ BLEU:              {generation_metrics['bleu']:.4f}")
        print(f"   üîç Semantic Similarity: {generation_metrics['semantic_similarity']:.4f}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìã –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        if retrieval_metrics['ndcg@10'] > 0.7:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        elif retrieval_metrics['ndcg@10'] > 0.5:
            print("   ‚úÖ –†–µ—Ç—Ä–∏–≤–µ—Ä: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞")
        else:
            print("   ‚ö†Ô∏è  –†–µ—Ç—Ä–∏–≤–µ—Ä: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

        if generation_metrics['rougeL'] > 0.7:
            print("   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –û–¢–õ–ò–ß–ù–û–ï –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
        elif generation_metrics['rougeL'] > 0.5:
            print("   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –•–û–†–û–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤")
        else:
            print("   ‚ö†Ô∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

    def _save_detailed_results(self, retrieval_results: List, generation_results: List,
                               retrieval_metrics: Dict, generation_metrics: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        try:
            results_dir = os.path.join(src_root, "evaluation")
            os.makedirs(results_dir, exist_ok=True)

            detailed_results = {
                'retrieval_metrics': retrieval_metrics,
                'generation_metrics': generation_metrics,
                'retrieval_details': retrieval_results,
                'generation_details': generation_results,
                'summary': {
                    'timestamp': np.datetime64('now').astype(str),
                    'total_questions': len(generation_results)
                }
            }

            results_path = os.path.join(results_dir, "detailed_metrics_results.json")
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, ensure_ascii=False, indent=2)

            print(f"\nüíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        except Exception as e:
            logger.error(f"Error saving detailed results: {e}")


def main():
    """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫"""
    try:
        evaluator = MetricsEvaluator()
        metrics = evaluator.evaluate_all_metrics()

        print(f"\nüéâ –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –ó–ê–í–ï–†–®–ï–ù–ê!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()