import streamlit as st
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Any
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import tempfile
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.append('src')

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
import evaluate
from bert_score import score as bert_score

# –ù–∞—à–∏ –º–æ–¥—É–ª–∏
from config import TransneftConfig, get_model_config, validate_benchmark_path
from benchmark_utils import BenchmarkAnalyzer, export_benchmark_report

class TransneftBenchmarkQA:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å QA —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å"""
    
    def __init__(self, benchmark_path: str = None):
        self.benchmark_path = benchmark_path or TransneftConfig.BENCHMARK_PATH
        self.benchmark_data = None
        self.vector_store = None
        self.qa_chain = None
        self.embeddings = None
        self.qa_pairs = []
        self.sections = []
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞
        self.load_benchmark()
        
    def load_benchmark(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        try:
            if not os.path.exists(self.benchmark_path):
                st.error(f"‚ùå –§–∞–π–ª –±–µ–Ω—á–º–∞—Ä–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.benchmark_path}")
                return
                
            with open(self.benchmark_path, 'r', encoding='utf-8') as f:
                self.benchmark_data = json.load(f)
            
            self.qa_pairs = self.benchmark_data.get('qa_pairs', [])
            self.sections = self.benchmark_data.get('sections', [])
            
            st.success(f"‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.qa_pairs)} QA –ø–∞—Ä, {len(self.sections)} —Å–µ–∫—Ü–∏–π")
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: {str(e)}")
    
    def initialize_system(self, embedding_model: str = None, search_mode: str = "balanced"):
        """–ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        if not self.benchmark_data:
            raise ValueError("–ë–µ–Ω—á–º–∞—Ä–∫ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model_name = embedding_model or TransneftConfig.EMBEDDING_MODELS["default"]
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = self.prepare_training_data()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.vector_store = FAISS.from_documents(documents, self.embeddings)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QA —Ü–µ–ø–∏
        self.initialize_qa_chain(search_mode)
        
        return len(documents)
    
    def prepare_training_data(self) -> List[Document]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        documents = []
        
        for qa_pair in self.qa_pairs:
            doc = Document(
                page_content=qa_pair['context'],
                metadata={
                    'section': qa_pair['section'],
                    'question_id': qa_pair['question_id'],
                    'source_file': qa_pair.get('source_file', ''),
                    'entities': qa_pair.get('entities', []),
                    'context_length': qa_pair.get('context_length', 0),
                    'word_count': qa_pair.get('word_count', 0)
                }
            )
            documents.append(doc)
        
        return documents
    
    def initialize_qa_chain(self, search_mode: str = "balanced", model_type: str = "local", api_key: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QA —Ü–µ–ø–∏"""
        search_config = get_model_config(search_mode)
        
        retriever = self.vector_store.as_retriever(
            search_type=search_config["search_type"],
            search_kwargs={k: v for k, v in search_config.items() if k != "search_type"}
        )
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–º—Ç–∞
        prompt_template = TransneftConfig.PROMPT_TEMPLATES["expert"]
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        if model_type == "openai" and api_key:
            llm = ChatOpenAI(
                model_name="gpt-3.5-turbo",
                openai_api_key=api_key,
                temperature=0.1,
                max_tokens=1000
            )
        else:
            # –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
            from langchain.llms import FakeListLLM
            responses = [
                "–ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –ü–ê–û '–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å'...",
                "–°–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...",
                "–í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ —É–∫–∞–∑–∞–Ω–æ..."
            ]
            llm = FakeListLLM(responses=responses)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ QA —Ü–µ–ø–∏
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–µ"""
        if not self.qa_chain:
            raise ValueError("QA —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        result = self.qa_chain({"query": question})
        return result
    
    def evaluate_system(self, sample_size: int = None) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ"""
        if not self.qa_chain:
            raise ValueError("QA —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –í—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        eval_qa_pairs = self.qa_pairs
        if sample_size and sample_size < len(self.qa_pairs):
            indices = np.random.choice(len(self.qa_pairs), sample_size, replace=False)
            eval_qa_pairs = [self.qa_pairs[i] for i in indices]
        
        generated_answers = []
        reference_answers = []
        questions = []
        sections = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, qa_pair in enumerate(eval_qa_pairs):
            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(eval_qa_pairs)}: {qa_pair['question_id']}")
            
            try:
                result = self.ask_question(qa_pair['question'])
                generated_answers.append(result['result'])
                reference_answers.append(qa_pair['answer'])
                questions.append(qa_pair['question'])
                sections.append(qa_pair['section'])
                
                progress_bar.progress((i + 1) / len(eval_qa_pairs))
                
            except Exception as e:
                st.warning(f"–ü—Ä–æ–ø—É—Å–∫ {qa_pair['question_id']}: {str(e)}")
                continue
        
        status_text.empty()
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        metrics = self.calculate_metrics(generated_answers, reference_answers)
        
        return {
            'metrics': metrics,
            'generated_answers': generated_answers,
            'reference_answers': reference_answers,
            'questions': questions,
            'sections': sections,
            'eval_qa_pairs': eval_qa_pairs
        }
    
    def calculate_metrics(self, generated_answers: List[str], reference_answers: List[str]) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
        # ROUGE
        rouge_scorer_obj = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=True
        )
        
        rouge_scores = []
        for gen, ref in zip(generated_answers, reference_answers):
            scores = rouge_scorer_obj.score(ref, gen)
            rouge_scores.append({
                'rouge1': scores['rouge1'].fmeasure,
                'rouge2': scores['rouge2'].fmeasure,
                'rougeL': scores['rougeL'].fmeasure
            })
        
        # BLEU
        bleu_scores = []
        for gen, ref in zip(generated_answers, reference_answers):
            reference = [ref.split()]
            candidate = gen.split()
            bleu_scores.append(sentence_bleu(reference, candidate))
        
        # BERTScore
        try:
            P, R, F1 = bert_score(generated_answers, reference_answers, lang="ru")
            bertscore_f1 = F1.mean().item()
        except:
            bertscore_f1 = 0.0
        
        # –ú–µ—Ç–µ–æ—Ä
        try:
            meteor = evaluate.load('meteor')
            meteor_score = meteor.compute(
                predictions=generated_answers, 
                references=reference_answers
            )['meteor']
        except:
            meteor_score = 0.0
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        rouge_df = pd.DataFrame(rouge_scores)
        
        return {
            'rouge1': rouge_df['rouge1'].mean(),
            'rouge2': rouge_df['rouge2'].mean(),
            'rougeL': rouge_df['rougeL'].mean(),
            'bleu': np.mean(bleu_scores),
            'bertscore': bertscore_f1,
            'meteor': meteor_score,
            'num_evaluated': len(generated_answers)
        }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    st.set_page_config(
        page_title="ü§ñ –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å QA –°–∏—Å—Ç–µ–º–∞",
        page_icon="üõ¢Ô∏è",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # –°—Ç–∏–ª–∏
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        border-left: 4px solid #1e3c72;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    st.markdown("""
    <div class="main-header">
        <h1>üõ¢Ô∏è AI –ß–∞—Ç-–±–æ—Ç –ü–ê–û "–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å"</h1>
        <h3>–≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    if 'qa_system' not in st.session_state:
        try:
            st.session_state.qa_system = TransneftBenchmarkQA()
            st.session_state.benchmark_analyzer = BenchmarkAnalyzer(TransneftConfig.BENCHMARK_PATH)
            
            with st.spinner("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã..."):
                doc_count = st.session_state.qa_system.initialize_system()
                
            st.session_state.system_ready = True
            st.success(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
            st.session_state.system_ready = False
    
    # –î–∞–ª—å–Ω–µ–π—à–∏–π –∫–æ–¥ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...
    # [–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∫–æ–¥–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞]

if __name__ == "__main__":
    main()