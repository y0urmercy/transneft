import os
import sys
import json
import numpy as np
import faiss
from typing import List, Dict, Tuple
import warnings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from utils.config import VECTOR_STORE_DIR

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
SAFE_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

class SimpleEmbeddings:
    """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ TensorFlow"""
    
    def __init__(self, model_name=SAFE_MODEL_NAME):
        print(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        
        try:
            # –ü—Ä—è–º–æ–π –∏–º–ø–æ—Ä—Ç transformers
            from transformers import AutoTokenizer, AutoModel
            import torch
            import torch.nn.functional as F
            
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
            self.tokenizer = AutoTokenizer.from_pretrained(SAFE_MODEL_NAME)
            self.model = AutoModel.from_pretrained(SAFE_MODEL_NAME)
            self.model.to(self.device)
            self.model.eval()
            
            self.torch = torch
            self.F = F
            
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ transformers: {e}")
            print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")
            raise
    
    def embed_documents(self, texts):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        embeddings = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        batch_size = 8
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self._embed_batch(batch_texts)
            embeddings.extend(batch_embeddings)
            
            if i % 32 == 0:
                print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, len(texts))}/{len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return np.array(embeddings)
    
    def embed_query(self, text):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        return self._embed_batch([text])[0]
    
    def _embed_batch(self, texts):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞"""
        with self.torch.no_grad():
            inputs = self.tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                max_length=512, 
                return_tensors="pt"
            ).to(self.device)
            
            outputs = self.model(**inputs)
            embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
            embeddings = self.F.normalize(embeddings, p=2, dim=1)
            
            return embeddings.cpu().numpy()
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è sentence embeddings"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return self.torch.sum(token_embeddings * input_mask_expanded, 1) / self.torch.clamp(input_mask_expanded.sum(1), min=1e-9)


class VectorStore:
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, model_name: str = SAFE_MODEL_NAME):
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorStore —Å –º–æ–¥–µ–ª—å—é: {model_name}")

        try:
            self.embeddings = SimpleEmbeddings(model_name)
            self.index = None
            self.chunks = []
            self.chunk_metadata = []
            self.is_initialized = False
            print("‚úÖ VectorStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ VectorStore: {e}")
            raise

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def create_embeddings(self, chunks: List[Dict]) -> None:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö chunks"""
        print(f"üì• –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} chunks...")

        if not chunks:
            raise ValueError("‚ùå –ù–µ—Ç chunks –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        self.chunks = [chunk['text'] for chunk in chunks]
        self.chunk_metadata = [chunk['metadata'] for chunk in chunks]

        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = self.embeddings.embed_documents(self.chunks)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤
            embeddings_np = np.array(embeddings).astype('float32')

            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            self.index = faiss.IndexFlatIP(embeddings_np.shape[1])
            self.index.add(embeddings_np)

            self.is_initialized = True
            print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise

    def search(self, query: str, k: int = 5, threshold: float = 0.3) -> List[Tuple[str, Dict, float]]:
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö chunks"""
        if not self.is_initialized or self.index is None:
            raise ValueError("‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ create_embeddings()")

        if not query or not query.strip():
            return []

        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embeddings.embed_query(query)
            query_embedding_np = np.array([query_embedding]).astype('float32')

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            scores, indices = self.index.search(query_embedding_np, min(k, len(self.chunks)))

            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.chunks) and score >= threshold:
                    results.append((
                        self.chunks[idx],
                        self.chunk_metadata[idx],
                        float(score)
                    ))

            return results

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def save_index(self, save_path: str = VECTOR_STORE_DIR):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        if not self.is_initialized:
            raise ValueError("‚ùå –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        
        try:
            os.makedirs(save_path, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(save_path, "faiss.index")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞: {index_path}")
            faiss.write_index(self.index, index_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunks_path = os.path.join(save_path, "chunks.json")
            metadata_path = os.path.join(save_path, "metadata.json")

            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)

            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)

            print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            raise

    def load_index(self, load_path: str = VECTOR_STORE_DIR):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            self.index = faiss.read_index(os.path.join(load_path, "faiss.index"))

            # –ó–∞–≥—Ä—É–∂–∞–µ–º chunks –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(os.path.join(load_path, "chunks.json"), "r", encoding="utf-8") as f:
                self.chunks = json.load(f)

            with open(os.path.join(load_path, "metadata.json"), "r", encoding="utf-8") as f:
                self.chunk_metadata = json.load(f)

            self.is_initialized = True
            print(f"üìÇ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {load_path}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise

    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if not self.is_initialized:
            return {"error": "–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ"}

        return {
            "total_chunks": len(self.chunks),
            "total_vectors": self.index.ntotal,
            "embedding_dimension": self.index.d,
            "device": self.embeddings.device
        }