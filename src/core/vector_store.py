import os
import sys
import json
import numpy as np
import faiss
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings("ignore")
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from utils.config import VECTOR_STORE_DIR
SAFE_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

class SimpleEmbeddings:
    """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ TensorFlow"""
    
    def __init__(self, model_name=SAFE_MODEL_NAME):
        print(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            import torch.nn.functional as F
            
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.tokenizer = AutoTokenizer.from_pretrained(SAFE_MODEL_NAME)
            self.model = AutoModel.from_pretrained(SAFE_MODEL_NAME)
            self.model.to(self.device)
            self.model.eval()
            
            self.torch = torch
            self.F = F
            
        except ImportError as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ transformers: {e}")
            print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")
            raise
    
    def embed_documents(self, texts):
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        embeddings = []
        batch_size = 8
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self._embed_batch(batch_texts)
            embeddings.extend(batch_embeddings)
            
            if i % 32 == 0:
                print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, len(texts))}/{len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return np.array(embeddings)
    
    def embed_query(self, text):
        return self._embed_batch([text])[0]
    
    def _embed_batch(self, texts):
        with self.torch.no_grad():
            inputs = self.tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                max_length=512, 
                return_tensors="pt"
            ).to(self.device)
            
            outputs = self.model(**inputs)
            embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
            embeddings = self.F.normalize(embeddings, p=2, dim=1)
            
            return embeddings.cpu().numpy()
    
    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return self.torch.sum(token_embeddings * input_mask_expanded, 1) / self.torch.clamp(input_mask_expanded.sum(1), min=1e-9)


class VectorStore:
    def __init__(self, model_name: str = SAFE_MODEL_NAME):
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorStore —Å –º–æ–¥–µ–ª—å—é: {model_name}")

        try:
            self.embeddings = SimpleEmbeddings(model_name)
            self.index = None
            self.chunks = []
            self.chunk_metadata = []
            self.is_initialized = False
            print("VectorStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ VectorStore: {e}")
            raise

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def create_embeddings(self, chunks: List[Dict]) -> None:
        if not chunks:
            raise ValueError("–ù–µ—Ç chunks –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        self.chunks = [chunk['text'] for chunk in chunks]
        self.chunk_metadata = [chunk['metadata'] for chunk in chunks]

        try:
            embeddings = self.embeddings.embed_documents(self.chunks)
            embeddings_np = np.array(embeddings).astype('float32')
            self.index = faiss.IndexFlatIP(embeddings_np.shape[1])
            self.index.add(embeddings_np)

            self.is_initialized = True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise

    def search(self, query: str, k: int = 5, threshold: float = 0.3) -> List[Tuple[str, Dict, float]]:
        if not self.is_initialized or self.index is None:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ create_embeddings()")

        if not query or not query.strip():
            return []

        try:
            query_embedding = self.embeddings.embed_query(query)
            query_embedding_np = np.array([query_embedding]).astype('float32')
            scores, indices = self.index.search(query_embedding_np, min(k, len(self.chunks)))

            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.chunks) and score >= threshold:
                    results.append((
                        self.chunks[idx],
                        self.chunk_metadata[idx],
                        float(score)
                    ))

            return results

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def save_index(self, save_path: str = VECTOR_STORE_DIR):
        if not self.is_initialized:
            raise ValueError("–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        
        try:
            os.makedirs(save_path, exist_ok=True)

            index_path = os.path.join(save_path, "faiss.index")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞: {index_path}")
            faiss.write_index(self.index, index_path)

            chunks_path = os.path.join(save_path, "chunks.json")
            metadata_path = os.path.join(save_path, "metadata.json")

            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)

            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)


        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            raise

    def load_index(self, load_path: str = VECTOR_STORE_DIR):
        try:
            self.index = faiss.read_index(os.path.join(load_path, "faiss.index"))
            with open(os.path.join(load_path, "chunks.json"), "r", encoding="utf-8") as f:
                self.chunks = json.load(f)

            with open(os.path.join(load_path, "metadata.json"), "r", encoding="utf-8") as f:
                self.chunk_metadata = json.load(f)

            self.is_initialized = True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise

    def get_stats(self) -> Dict:
        if not self.is_initialized:
            return {"error": "–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ"}

        return {
            "total_chunks": len(self.chunks),
            "total_vectors": self.index.ntotal,
            "embedding_dimension": self.index.d,
            "device": self.embeddings.device
        }