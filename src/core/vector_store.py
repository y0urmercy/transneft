import os
import sys
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import torch
from typing import List, Dict, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
src_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, src_root)

from utils.config import MODEL_NAME, VECTOR_STORE_DIR, EMBEDDING_DIMENSION


class VectorStore:
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, model_name: str = MODEL_NAME):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VectorStore –Ω–∞ {self.device}")

        try:
            self.model = SentenceTransformer(model_name, device=self.device)
            self.index = None
            self.chunks = []
            self.chunk_metadata = []
            self.is_initialized = False
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def create_embeddings(self, chunks: List[Dict]) -> None:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö chunks"""
        print(" –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

        if not chunks:
            raise ValueError(" –ù–µ—Ç chunks –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        self.chunks = [chunk['text'] for chunk in chunks]
        self.chunk_metadata = [chunk['metadata'] for chunk in chunks]

        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            embeddings = self.model.encode(
                self.chunks,
                batch_size=16,
                show_progress_bar=True,
                convert_to_tensor=True,
                normalize_embeddings=True
            )

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
            embeddings_np = embeddings.cpu().numpy() if self.device == "cuda" else embeddings.numpy()

            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            self.index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
            self.index.add(embeddings_np)

            self.is_initialized = True
            print(f" –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise

    def search(self, query: str, k: int = 5, threshold: float = 0.3) -> List[Tuple[str, Dict, float]]:
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö chunks"""
        if not self.is_initialized or self.index is None:
            raise ValueError(" –ò–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ create_embeddings()")

        if not query or not query.strip():
            return []

        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.model.encode(
                [query],
                convert_to_tensor=True,
                normalize_embeddings=True
            )

            query_embedding_np = query_embedding.cpu().numpy() if self.device == "cuda" else query_embedding.numpy()

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            scores, indices = self.index.search(query_embedding_np, k)

            results = []
            for score, idx in zip(scores[0], indices[0]):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∏ –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                if idx < len(self.chunks) and score >= threshold:
                    results.append((
                        self.chunks[idx],
                        self.chunk_metadata[idx],
                        float(score)
                    ))

            return results

        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def save_index(self, save_path: str = VECTOR_STORE_DIR):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        if not self.is_initialized:
            raise ValueError(" –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        try:
            # –Ø–≤–Ω–æ —Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
            os.makedirs(save_path, exist_ok=True)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
            if not os.path.exists(save_path):
                raise OSError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {save_path}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å
            test_file = os.path.join(save_path, "test_write.tmp")
            try:
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
            except Exception as e:
                raise OSError(f"–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {save_path}: {e}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(save_path, "faiss.index")
            print(f" –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞: {index_path}")
            faiss.write_index(self.index, index_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunks_path = os.path.join(save_path, "chunks.json")
            metadata_path = os.path.join(save_path, "metadata.json")
            model_info_path = os.path.join(save_path, "model_info.json")

            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)

            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_info = {
                "model_name": MODEL_NAME,
                "embedding_dimension": EMBEDDING_DIMENSION,
                "total_vectors": self.index.ntotal,
                "device": self.device
            }

            with open(model_info_path, "w", encoding="utf-8") as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)

            print(f" –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")

        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")

            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å
            print(" –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é...")
            self._save_to_alternative_location()

    def _save_to_alternative_location(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            alt_path = "vector_store_temp"
            os.makedirs(alt_path, exist_ok=True)

            faiss.write_index(self.index, f"{alt_path}/faiss.index")

            with open(f"{alt_path}/chunks.json", "w", encoding="utf-8") as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)

            with open(f"{alt_path}/metadata.json", "w", encoding="utf-8") as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)

            print(f" –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {alt_path}")
            return alt_path

        except Exception as e:
            print(f" –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
            raise

    def load_index(self, load_path: str = VECTOR_STORE_DIR):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
            required_files = [
                os.path.join(load_path, "faiss.index"),
                os.path.join(load_path, "chunks.json"),
                os.path.join(load_path, "metadata.json")
            ]

            for file_path in required_files:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            self.index = faiss.read_index(os.path.join(load_path, "faiss.index"))

            # –ó–∞–≥—Ä—É–∂–∞–µ–º chunks –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(os.path.join(load_path, "chunks.json"), "r", encoding="utf-8") as f:
                self.chunks = json.load(f)

            with open(os.path.join(load_path, "metadata.json"), "r", encoding="utf-8") as f:
                self.chunk_metadata = json.load(f)

            self.is_initialized = True
            print(f" –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {load_path}")
            print(f" –†–∞–∑–º–µ—Ä: {len(self.chunks)} chunks, {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
            raise

    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if not self.is_initialized:
            return {"error": "–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ"}

        return {
            "total_chunks": len(self.chunks),
            "total_vectors": self.index.ntotal,
            "embedding_dimension": EMBEDDING_DIMENSION,
            "device": self.device,
            "model": MODEL_NAME
        }


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    from utils.config import CHUNKS_PATH

    with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:
        chunks = json.load(f)

    vector_store = VectorStore()
    vector_store.create_embeddings(chunks)
    vector_store.save_index()

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    test_queries = [
        "–°–∫–æ–ª—å–∫–æ –∞–∫—Ü–∏–π –≤ —É—Å—Ç–∞–≤–Ω–æ–º –∫–∞–ø–∏—Ç–∞–ª–µ?",
        "–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
        "–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏"
    ]

    print("\n –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–ê:")
    for query in test_queries:
        print(f"\n–ó–∞–ø—Ä–æ—Å: '{query}'")
        results = vector_store.search(query, k=2)
        for i, (chunk, metadata, score) in enumerate(results):
            print(f"  {i + 1}. [Score: {score:.3f}] {chunk[:80]}...")
